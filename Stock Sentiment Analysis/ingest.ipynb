{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion Layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data ingestion layer will receive the collected data and prepare it for storage and analysis. \n",
    "This process cleans the text, normalizes it, and splits it into manageable chunks for storage and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# social_media_data = pd.read_pickle(\"social_media_data.pkl\")\n",
    "with open('files/social_media_data.json', 'r') as file:\n",
    "    social_media_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags, special characters, and extra whitespace from text.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The input text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "    str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z.A-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text by converting to lowercase.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    str: The normalized text.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def clean_and_preprocess(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        # Remove HTML tags, special characters, etc.\n",
    "        cleaned_text = clean_text(str(item[\"page_content\"]))\n",
    "        # Normalize text (lowercase, remove extra whitespace)\n",
    "        normalized_text = normalize_text(cleaned_text)\n",
    "        # Create a new Document with cleaned and normalized text\n",
    "        # cleaned_data.append(Document(page_content=normalized_text))\n",
    "        cleaned_data.append(Document(page_content=normalized_text, \n",
    "                                     metadata={\"platform\":item[\"platform\"],\"company\":item[\"company\"]}))\n",
    "    return cleaned_data\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "def add_metadata(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Add additional metadata to the documents.\n",
    "    \n",
    "    Args:\n",
    "    documents (List[Document]): List of Document objects.\n",
    "    \n",
    "    Returns:\n",
    "    List[Document]: List of Document objects with additional metadata.\n",
    "    \"\"\"\n",
    "    for doc in documents:\n",
    "        doc.metadata['ingestion_timestamp'] = datetime.now().isoformat()\n",
    "        doc.metadata['word_count'] = len(doc.page_content.split())\n",
    "    return documents\n",
    "\n",
    "def validate_data(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Validate the data and remove any invalid documents.\n",
    "    \n",
    "    Args:\n",
    "    documents (List[Document]): List of Document objects to be validated.\n",
    "    \n",
    "    Returns:\n",
    "    List[Document]: List of valid Document objects.\n",
    "    \"\"\"\n",
    "    valid_documents = []\n",
    "    for doc in documents:\n",
    "        if len(doc.page_content.split()) > 5:  # Ensure document has more than 5 words\n",
    "            if all(key in doc.metadata for key in ['platform', 'company']):  # Ensure required metadata is present\n",
    "                valid_documents.append(doc)\n",
    "    return valid_documents\n",
    "\n",
    "\n",
    "def ingest_data(data):\n",
    "    cleaned_data = clean_and_preprocess(data)\n",
    "    split_data = split_documents(cleaned_data)\n",
    "    enriched_data = add_metadata(split_data)\n",
    "    valid_data = validate_data(enriched_data)\n",
    "    return valid_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingested_data = ingest_data(social_media_data)\n",
    "# display(ingested_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a file\n",
    "with open('files/ingested_data.pkl', 'wb') as file:\n",
    "    pickle.dump(ingested_data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
