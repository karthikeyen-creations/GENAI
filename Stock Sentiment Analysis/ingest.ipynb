{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion Layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data ingestion layer will receive the collected data and prepare it for storage and analysis. \n",
    "This process cleans the text, normalizes it, and splits it into manageable chunks for storage and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_data = pd.read_pickle(\"social_media_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags, special characters, and extra whitespace from text.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The input text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "    str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text by converting to lowercase.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The input text to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    str: The normalized text.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def clean_and_preprocess(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        # Remove HTML tags, special characters, etc.\n",
    "        cleaned_text = clean_text(item.page_content)\n",
    "        # Normalize text (lowercase, remove extra whitespace)\n",
    "        normalized_text = normalize_text(cleaned_text)\n",
    "        # Create a new Document with cleaned and normalized text\n",
    "        cleaned_data.append(Document(page_content=normalized_text, metadata=item.metadata))\n",
    "    return cleaned_data\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "def add_metadata(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Add additional metadata to the documents.\n",
    "    \n",
    "    Args:\n",
    "    documents (List[Document]): List of Document objects.\n",
    "    \n",
    "    Returns:\n",
    "    List[Document]: List of Document objects with additional metadata.\n",
    "    \"\"\"\n",
    "    for doc in documents:\n",
    "        doc.metadata['ingestion_timestamp'] = datetime.now().isoformat()\n",
    "        doc.metadata['word_count'] = len(doc.page_content.split())\n",
    "    return documents\n",
    "\n",
    "def validate_data(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Validate the data and remove any invalid documents.\n",
    "    \n",
    "    Args:\n",
    "    documents (List[Document]): List of Document objects to be validated.\n",
    "    \n",
    "    Returns:\n",
    "    List[Document]: List of valid Document objects.\n",
    "    \"\"\"\n",
    "    valid_documents = []\n",
    "    for doc in documents:\n",
    "        if len(doc.page_content.split()) > 5:  # Ensure document has more than 5 words\n",
    "            if all(key in doc.metadata for key in ['source', 'id']):  # Ensure required metadata is present\n",
    "                valid_documents.append(doc)\n",
    "    return valid_documents\n",
    "\n",
    "\n",
    "def ingest_data(data):\n",
    "    cleaned_data = clean_and_preprocess(data)\n",
    "    split_data = split_documents(cleaned_data)\n",
    "    enriched_data = add_metadata(split_data)\n",
    "    valid_data = validate_data(enriched_data)\n",
    "    return valid_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ingested_data \u001b[38;5;241m=\u001b[39m \u001b[43mingest_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msocial_media_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 85\u001b[0m, in \u001b[0;36mingest_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mingest_data\u001b[39m(data):\n\u001b[1;32m---> 85\u001b[0m     cleaned_data \u001b[38;5;241m=\u001b[39m \u001b[43mclean_and_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     split_data \u001b[38;5;241m=\u001b[39m split_documents(cleaned_data)\n\u001b[0;32m     87\u001b[0m     enriched_data \u001b[38;5;241m=\u001b[39m add_metadata(split_data)\n",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m, in \u001b[0;36mclean_and_preprocess\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     36\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Remove HTML tags, special characters, etc.\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m clean_text(\u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Normalize text (lowercase, remove extra whitespace)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     normalized_text \u001b[38;5;241m=\u001b[39m normalize_text(cleaned_text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "ingested_data = ingest_data(social_media_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
